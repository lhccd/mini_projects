{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSS_SemanticDrift_lorenz.ipynb",
      "provenance": [],
      "mount_file_id": "1h756aZJHq2FjPVacQ74fjrl72WqvVwkW",
      "authorship_tag": "ABX9TyP21AvuSwSHlOCMkFDY6+by",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lhccd/mini_projects/blob/main/CSS_SemanticDrift_lorenz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aKaedB5iPE8J"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/putin_jan_2010_to_jan_2014.json', 'r') as f:\n",
        "  putin_data_2010_2014 = json.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/putin_feb_2022_to_jun_2022.json', 'r') as f:\n",
        "  putin_data_2022_2022 = json.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(putin_data_2010_2014[:100])\n",
        "#print(putin_data_2022_2022[:100])\n",
        "#for i, j in enumerate(putin_data_2022_2022):\n",
        "#  print(i, j[\"tweet_text\"])\n",
        "#  if(i==20):\n",
        "#    break"
      ],
      "metadata": {
        "id": "Km7iskqkATr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# THIS SNIPPET REMOVES ALL HYPERLINKS FROM THE TWEETS \n",
        "\n",
        "import re \n",
        "\n",
        "txt = \"\"\n",
        "#tweets = []\n",
        "for e in putin_data_2010_2014: \n",
        "  tweet = e[\"tweet_text\"]\n",
        "  tweet = re.sub(r\"\"\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)\"\"\", \"\", tweet)\n",
        "  tweet = tweet + \" \"\n",
        "  #tweets.append(tweet)\n",
        "  txt = txt + tweet\n",
        "\n",
        "print(txt[:1000])\n",
        "#for i,e in enumerate(putin_data_2010_2014[:30]):\n",
        "#  print(\"*********************\")\n",
        "#  print(e[\"tweet_text\"])\n",
        "#  print(tweets[i])\n",
        "#  print(\"*********************\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHuL4jwpPYFc",
        "outputId": "c99d2315-ef4a-4899-a913-9930ae4accce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "newStream©: Putin Vows to Destroy Terrorists After Bombings  Putin putin putin putin putin #Putin \"We will continue the fight against terrorists until their complete annihilation\" as Syria's children continue to die by his weapons. @trscoop More importantly, it blunted the point CK was making, that Putin is a winner mainly because Obama was so incompetent. Before comparing Putin and Obama, you have to remember that Putin loves his country and will do anything :// Casse les couilles putin We will destroy you: Putin sends starkest message to terrorists - The Times (subscription)  Putin announced gibber nominal value syntactic analysis with ukrainian.: dIZk UK paper names #Putin International Person of Year  Russia's Putin promises to annihilate terrorists after deadly bombings:  Russia's Putin vows in New Year speech to annihilate 'terrorists': Russia's president Vladimir Putin has used ...  2013 winners, Palin, Cruz, Gowdy, Gohmert, Lee, Putin, Krauthammer, Beck, Limbaugh, &amp; last bu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# perform data cleaning\n",
        "## TODO: remove links/ user mentions\n",
        "\n",
        "# lower case txt\n",
        "txt = txt.lower()\n",
        "\n",
        "# remove emojis\n",
        "import re\n",
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',text)\n",
        "\n",
        "txt = deEmojify(txt)"
      ],
      "metadata": {
        "id": "wnkPfqSRPahw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save text file\n",
        "\n",
        "#open text file\n",
        "text_file = open('/content/train.txt', 'w')\n",
        " \n",
        "#write string to file\n",
        "text_file.write(txt)\n",
        " \n",
        "#close file\n",
        "text_file.close()"
      ],
      "metadata": {
        "id": "4itvov4KPccH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PJcnUZLgt3t5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Python program to generate word vectors using Word2Vec\n",
        " \n",
        "# importing all necessary modules\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        " \n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "#  Reads ‘train.txt’ file\n",
        "sample = open('/content/train.txt', 'r')\n",
        "s = sample.read()\n",
        " \n",
        "# Replaces escape character with space\n",
        "f = s.replace(\"\\n\", \" \")\n",
        " \n",
        "data = []\n",
        " \n",
        "# iterate through each sentence in the file\n",
        "for i in sent_tokenize(f):\n",
        "    temp = []\n",
        "     \n",
        "    # tokenize the sentence into words\n",
        "    for j in word_tokenize(i):\n",
        "        temp.append(j.lower())\n",
        " \n",
        "    data.append(temp)\n",
        " \n",
        "# Create CBOW model\n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5)\n",
        " \n",
        "# Print results\n",
        "print(\"Cosine similarity between 'putin' \" +\n",
        "               \"and 'ukraine' - CBOW : \",\n",
        "    model1.wv.similarity('putin', 'ukraine'))\n",
        "     \n",
        "print(\"Cosine similarity between 'putin' \" +\n",
        "                 \"and 'hate' - CBOW : \",\n",
        "      model1.wv.similarity('putin', 'hate'))\n",
        "\n",
        "print(\"Cosine similarity between 'putin' \" +\n",
        "                 \"and 'love' - CBOW : \",\n",
        "      model1.wv.similarity('putin', 'love'))\n",
        " \n",
        "# Create Skip Gram model\n",
        "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5, sg = 1)\n",
        " \n",
        "# Print results\n",
        "print(\"Cosine similarity between 'putin' \" +\n",
        "          \"and 'ukraine' - Skip Gram : \",\n",
        "    model2.wv.similarity('putin', 'ukraine'))\n",
        "     \n",
        "print(\"Cosine similarity between 'putin' \" +\n",
        "            \"and 'hate' - Skip Gram : \",\n",
        "      model2.wv.similarity('putin', 'hate'))    \n",
        " \n",
        "print(\"Cosine similarity between 'putin' \" +\n",
        "            \"and 'love' - Skip Gram : \",\n",
        "      model2.wv.similarity('putin', 'love'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFnOBp_kuC2r",
        "outputId": "96b50dc8-e8dc-4538-b296-9b959e7b1fb1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'putin' and 'ukraine' - CBOW :  0.29668373\n",
            "Cosine similarity between 'putin' and 'hate' - CBOW :  -0.097662956\n",
            "Cosine similarity between 'putin' and 'love' - CBOW :  0.04520077\n",
            "Cosine similarity between 'putin' and 'ukraine' - Skip Gram :  0.4206067\n",
            "Cosine similarity between 'putin' and 'hate' - Skip Gram :  0.56872576\n",
            "Cosine similarity between 'putin' and 'love' - Skip Gram :  0.42165282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Create CBOW model\n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5)\n",
        " \n",
        "# Print results\n",
        "print(\"Cosine similarity between 'putin' \" +\n",
        "               \"and 'ukraine' - CBOW : \",\n",
        "    model1.wv.similarity('putin', 'ukraine'))\n",
        "     \n",
        "print(\"Cosine similarity between 'putin' \" +\n",
        "                 \"and 'hate' - CBOW : \",\n",
        "      model1.wv.similarity('putin', 'hate'))\n",
        "\n",
        "print(\"Cosine similarity between 'putin' \" +\n",
        "                 \"and 'love' - CBOW : \",\n",
        "      model1.wv.similarity('putin', 'love'))\n",
        " \n",
        "# Create Skip Gram model\n",
        "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100,\n",
        "                                             window = 5, sg = 1)\n",
        " \n",
        "# Print results\n",
        "print(\"Cosine similarity between 'putin' \" +\n",
        "          \"and 'ukraine' - Skip Gram : \",\n",
        "    model2.wv.similarity('putin', 'ukraine'))\n",
        "     \n",
        "print(\"Cosine similarity between 'putin' \" +\n",
        "            \"and 'hate' - Skip Gram : \",\n",
        "      model2.wv.similarity('putin', 'hate'))    \n",
        " \n",
        "print(\"Cosine similarity between 'putin' \" +\n",
        "            \"and 'love' - Skip Gram : \",\n",
        "      model2.wv.similarity('putin', 'love'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "alS0vrf7vQq2",
        "outputId": "e37e0741-5192-48c9-b3bd-3c90ccd78cee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a8ec2353dcf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create CBOW model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Print results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gensim.__version__ )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USpDyKO23Lul",
        "outputId": "cf7385cf-9131-4f80-ba9e-503206fb7f91"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "2DoIqw4L3-9D",
        "outputId": "ee1464dc-6065-426b-8c26-d60c74bea693"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 320 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*****AB HIER ALLES UNWICHTIG*****"
      ],
      "metadata": {
        "id": "t6e07j3qvmIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l8gZmk6zvpHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWebnE0FVAUt",
        "outputId": "9b99c2de-40ea-445f-a6fd-e5f57d39203a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "with open('/content/train.txt', 'r') as f:\n",
        "  tweets_text = f.read()\n",
        "\n",
        "print(tweets_text[:1000])\n",
        "testset = tweets_text[:10000]\n",
        "word_tokenize(testset)"
      ],
      "metadata": {
        "id": "RRcuOpDeVKY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "\n",
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer"
      ],
      "metadata": {
        "id": "WYWFo-DHpjaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paths = ['/content/train.txt']\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])\n",
        "\n",
        "!mkdir Ukraine_ML\n",
        "tokenizer.save_model(\"Ukraine_ML\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oAuG4x4pyUR",
        "outputId": "9f8a9ddc-daec-4873-ca51-a8fbbf551197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ukraine_ML/vocab.json', 'Ukraine_ML/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"./Ukraine_ML/vocab.json\",\n",
        "    \"./Ukraine_ML/merges.txt\",\n",
        ")"
      ],
      "metadata": {
        "id": "lRvTLVFCs594"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)"
      ],
      "metadata": {
        "id": "QPQiwaXHtASL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"This is putin\").tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4afLTEWqtEVo",
        "outputId": "194af973-3222-4230-b4ee-44e4c1c8a898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', 'T', 'his', 'Ġis', 'Ġputin', '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Language Model"
      ],
      "metadata": {
        "id": "u-kQKzTItLIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjAfglsmtLyi",
        "outputId": "bd661fd8-0806-415b-f8ae-41f92671bea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")"
      ],
      "metadata": {
        "id": "QjCS1ryFtQSi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}